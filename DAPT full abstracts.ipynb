{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gn7AWxNDLhkN"
   },
   "source": [
    "# Install and import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skxpVe7yWYBm"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2bmwFAMW0mI"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils import data \n",
    "from sklearn.model_selection import train_test_split\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_MASKED_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    "    RobertaModel, \n",
    "    RobertaTokenizerFast, \n",
    "    RobertaForMaskedLM\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint, is_main_process\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "from matplotlib import rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EqzrkKPGYME2"
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NqijFrllLm_g"
   },
   "source": [
    "# Check for GPU\n",
    "\n",
    "Tesla P100 is recommended since this procedure takes a few hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lepX8X-QW6hW"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jv6P74VaXda4"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQorbMdYXfvc"
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N01bgNP3Mqhl"
   },
   "source": [
    "# Prepare Data\n",
    "\n",
    "\n",
    "*   load unlabeled dataset with >31,500 abstracts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VGZxcagpbJCv"
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(\"pubmed_papers.json\")\n",
    "df = df.drop(columns=[\"Doi\", \"PMID\", \"Extractive\"])\n",
    "df = df.rename(columns={\"Sentences\": \"text\"})\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1AuwMT1SlWL7"
   },
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddLVRlAVHPoE"
   },
   "outputs": [],
   "source": [
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QIwkf7U-neqq"
   },
   "outputs": [],
   "source": [
    "# transform abstract from sentence list to single string\n",
    "df_train[\"text\"] = df_train[\"text\"].apply(lambda x: (\" \").join(x))\n",
    "df_test[\"text\"] = df_test[\"text\"].apply(lambda x: (\" \").join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Xeei9VaOSSU"
   },
   "outputs": [],
   "source": [
    "dataset_train = Dataset.from_pandas(df_train)\n",
    "dataset_test = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nh-kAv1QNcD1"
   },
   "source": [
    "# Define model and tokenization function\n",
    "\n",
    "This implementation uses a lot of the prepared functions from the hugginface library which makes the training process easier. \n",
    "\n",
    "The tokenization function uses the same special token positioning as for the fine-tuning task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kO_pZoLgcywT"
   },
   "outputs": [],
   "source": [
    "PRE_TRAINED_MODEL_NAME = \"distilbert-base-uncased\" \n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(PRE_TRAINED_MODEL_NAME) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xL9uS5TFY4LA"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUWWX9FLZfIc"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qOX5oklJVZwO"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "  input_ids = []\n",
    "  attention_mask = []\n",
    "  special_tokens_mask = []\n",
    "  for sentence in examples[\"text\"]:\n",
    "    encoded = tokenizer(\n",
    "        sentence,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_special_tokens_mask=True\n",
    "    )\n",
    "    input_ids.extend(encoded[\"input_ids\"])\n",
    "    attention_mask.extend(encoded[\"attention_mask\"])\n",
    "    special_tokens_mask.extend(encoded[\"special_tokens_mask\"])\n",
    "  if len(input_ids) > MAX_LEN:\n",
    "    input_ids = input_ids[:MAX_LEN]\n",
    "    attention_mask = attention_mask[:MAX_LEN]\n",
    "    special_tokens_mask = special_tokens_mask[:MAX_LEN]\n",
    "    input_ids[MAX_LEN-1] = tokenizer.sep_token_id\n",
    "    special_tokens_mask[MAX_LEN-1] = 1\n",
    "  elif len(input_ids) < MAX_LEN:\n",
    "    padding_ids = [1] * (MAX_LEN - len(input_ids))\n",
    "    padding_attn = [0] * (MAX_LEN - len(input_ids))\n",
    "    input_ids.extend(padding_ids)\n",
    "    attention_mask.extend(padding_attn)\n",
    "    special_tokens_mask.extend(padding_ids)\n",
    "  \n",
    "  return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"attention_mask\": attention_mask,\n",
    "      \"special_tokens_mask\": special_tokens_mask \n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHjq5j7vQpkx"
   },
   "outputs": [],
   "source": [
    "tokenized_train = dataset_train.map(\n",
    "    tokenize_function,\n",
    "    remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2xV2rCNRT_x"
   },
   "outputs": [],
   "source": [
    "tokenized_test = dataset_test.map(\n",
    "    tokenize_function,\n",
    "    remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grjXmoAebh1Q"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIIpAl0lPTko"
   },
   "source": [
    "# Training and validation\n",
    "\n",
    "Here the actual DAPT happens. The hyperparameters for training must be defined and then the Trainer from huggingface does the work for us. During the training process some metrics are returned after every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Wp7OC_XhGQS"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir = \"\",\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    per_device_train_batch_size = 10,\n",
    "    per_device_eval_batch_size = 10,\n",
    "    learning_rate = 6e-4, #1e-4, 3e-5\n",
    "    weight_decay = 0.01,\n",
    "    adam_beta2 = 0.98,\n",
    "    adam_epsilon = 1e-6,\n",
    "    max_grad_norm = 0.0,\n",
    "    num_train_epochs = 10.0,\n",
    "    warmup_ratio = 0.06,\n",
    "    save_steps=5000,\n",
    "    seed = RANDOM_SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6TCAqpvdDSm"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTeH_eJ0bKeT"
   },
   "outputs": [],
   "source": [
    "last_checkpoint = None \n",
    "model_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31maBjvkayoz"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if last_checkpoint is not None:\n",
    "  checkpoint = last_checkpoint\n",
    "elif model_path is not None:\n",
    "  checkpoint = model_path\n",
    "else:\n",
    "  checkpoint = None\n",
    "train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "metrics = train_result.metrics\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bw5fs49GsBiK"
   },
   "outputs": [],
   "source": [
    "model.distilbert.save_pretrained(\"distilbert_dapt\") "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DAPT full abstracts.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
