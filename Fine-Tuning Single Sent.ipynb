{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLhsp6u3M9zh"
   },
   "source": [
    "# Install and import necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NFB0ilFFaJHz"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h-Cw5qTIaPUT"
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizerFast, DistilBertTokenizerFast, DistilBertModel, RobertaModel, RobertaTokenizerFast\n",
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "import torch \n",
    "import copy\n",
    "import math\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import textwrap \n",
    "from pylab import rcParams\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch import nn, optim\n",
    "from torch.utils import data \n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D8hklJioNNZh"
   },
   "source": [
    "# Check for GPU\n",
    "\n",
    "Tesla T4 and P100 should work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NC1xNu8zaQub"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQcsuVRIaUMg"
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qq-rh2x9aXBK"
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SR6aiE_ENSHc"
   },
   "source": [
    "# Prepare Data\n",
    "This works a bit differently since we need single sentences as input, but still want to preserve the same datasplits.\n",
    "*   open labeled dataset as dataframe\n",
    "*   add abstract id so we know to which abstract each sentences belongs\n",
    "*   create datasplits with random seed\n",
    "*   select a split by changing the idx\n",
    "*   explode the entries in the dataframe to have an entry for each sentence \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57DErbL7aZH8"
   },
   "outputs": [],
   "source": [
    "df = pd.read_json(\"cleaned1000.json\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SuR5qmnj9H2C"
   },
   "outputs": [],
   "source": [
    "df['Abstract_index'] = df.index\n",
    "df = df[[\"Sentences\", \"Extractive\", \"Abstract_index\"]]\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7G2t2SIcQbFT"
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "train_idx = []\n",
    "test_idx = []\n",
    "for train_index, test_index in kf.split(df):\n",
    "  train_idx.append(train_index)\n",
    "  test_idx.append(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHgpB3FwTyWY"
   },
   "outputs": [],
   "source": [
    "idx = 2\n",
    "df_train = df.iloc[train_idx[idx]]\n",
    "df_test = df.iloc[test_idx[idx]]\n",
    "df_val = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2D27AqJ1vT5"
   },
   "outputs": [],
   "source": [
    "# this function to explode to columns at the same times is from: https://stackoverflow.com/questions/12680754/split-explode-pandas-dataframe-string-entry-to-separate-rows\n",
    "def explode(df, lst_cols, fill_value='', preserve_index=False):\n",
    "    # make sure `lst_cols` is list-alike\n",
    "    if (lst_cols is not None\n",
    "        and len(lst_cols) > 0\n",
    "        and not isinstance(lst_cols, (list, tuple, np.ndarray, pd.Series))):\n",
    "        lst_cols = [lst_cols]\n",
    "    # all columns except `lst_cols`\n",
    "    idx_cols = df.columns.difference(lst_cols)\n",
    "    # calculate lengths of lists\n",
    "    lens = df[lst_cols[0]].str.len()\n",
    "    # preserve original index values    \n",
    "    idx = np.repeat(df.index.values, lens)\n",
    "    # create \"exploded\" DF\n",
    "    res = (pd.DataFrame({\n",
    "                col:np.repeat(df[col].values, lens)\n",
    "                for col in idx_cols},\n",
    "                index=idx)\n",
    "             .assign(**{col:np.concatenate(df.loc[lens>0, col].values)\n",
    "                            for col in lst_cols}))\n",
    "    # append those rows that have empty lists\n",
    "    if (lens == 0).any():\n",
    "        # at least one list in cells is empty\n",
    "        res = (res.append(df.loc[lens==0, idx_cols], sort=False)\n",
    "                  .fillna(fill_value))\n",
    "    # revert the original index order\n",
    "    res = res.sort_index()\n",
    "    # reset index if requested\n",
    "    if not preserve_index:        \n",
    "        res = res.reset_index(drop=True)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3CxbRUU-4mG7"
   },
   "outputs": [],
   "source": [
    "df_train = explode(df_train, [\"Sentences\", \"Extractive\"], fill_value='')\n",
    "df_test = explode(df_test, [\"Sentences\", \"Extractive\"], fill_value='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pd-cuPA8_8XT"
   },
   "source": [
    "# Chose model and tokenizer\n",
    "Replace the model name with one of the commented names to select on of the following models and its tokenizer from huggingface or local data:\n",
    "\n",
    "\n",
    "*   BERT-cased/uncased\n",
    "*   DistilBERT-cased/uncased\n",
    "*   RoBERTa\n",
    "*   BioBERT\n",
    "*   SciBERT-scivocab-cased/uncased (basevocab versions must be downloaded manually from the repository (https://github.com/allenai/scibert)\n",
    "*   Bert-cased DAPT\n",
    "*   DistilBERT-uncased DAPT\n",
    "\n",
    "\n",
    "For the DAPT models the tokenizers of the base model needs to be selected. Notice that for single sentence input I only made one notebook for all models because there aren't as many changes necessary when switching models. However, when switching between BERT, DistilBERT, and RoBERTa, some entries in the code need to be adjusted. These entries are marked with comments\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6f2yEsuakZ2"
   },
   "outputs": [],
   "source": [
    "# \"bert-base-cased\"\n",
    "# \"bert-base-uncased\"\n",
    "# \"distilbert-base-cased\"\n",
    "# \"distilbert-based-uncased\"\n",
    "# \"roberta-base\"\n",
    "# \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "# \"bionlp/bluebert_pubmed_uncased_L-12_H-768_A-12\"\n",
    "# \"allenai/scibert_scivocab_uncased\"\n",
    "# \"allenai/scibert_scivocab_cased\"  \n",
    "# \"/content/drive/MyDrive/Master Thesis/DAPT/distilbert_DAPT\"\n",
    "\n",
    "\n",
    "PRE_TRAINED_MODEL_NAME = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(PRE_TRAINED_MODEL_NAME) #DistilBertTokenizerFast / RobertaTokenizerFast\n",
    "#tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUQ3o5M0A0IE"
   },
   "source": [
    "# Tokenizer and DataLoaders\n",
    "\n",
    "The tokenizer creates the input for the pre-trained LMs and does the padding/truncation. This is simpler for single sentence input. The necessary outputs are:\n",
    "\n",
    "*   Labels\n",
    "*   Standard inputs: input_ids, attention_mask (segment_ids aren't necessary for BERT here because each input has only one sentence)\n",
    "\n",
    "The dataloaders use this function to create batches from the dataset which can be used by the LMs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpqI-x7wzIXt"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 64\n",
    "BATCH_SIZE = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7DOdE_yzagj"
   },
   "outputs": [],
   "source": [
    "class SentenceDataset(data.Dataset):\n",
    "\n",
    "  def __init__(self, sentences, targets, abstract_ids, tokenizer, max_len):\n",
    "    self.sentences = sentences\n",
    "    self.targets = targets\n",
    "    self.abstract_ids = abstract_ids\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.sentences)\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    sentence = self.sentences[item]\n",
    "    target = self.targets[item]\n",
    "    abstract_id = self.abstract_ids[item]\n",
    "\n",
    "    encoding = tokenizer(\n",
    "      sentence,\n",
    "      padding=\"max_length\", \n",
    "      truncation=True, \n",
    "      max_length=self.max_len,   \n",
    "      return_tensors=\"pt\" \n",
    "    )\n",
    "\n",
    "    return {\n",
    "      \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "      \"labels\": torch.tensor(target, dtype=torch.long),\n",
    "      \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "      \"abstract_ids\": torch.tensor(abstract_id, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2Rwn6L703s5"
   },
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  ds = SentenceDataset(\n",
    "      sentences = df.Sentences.to_numpy(),\n",
    "      targets = df.Extractive.to_numpy(),\n",
    "      abstract_ids = df.Abstract_index.to_numpy(),\n",
    "      tokenizer=tokenizer,\n",
    "      max_len=max_len,\n",
    "  )\n",
    "\n",
    "  return data.DataLoader(\n",
    "      ds,\n",
    "      batch_size=batch_size,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXvA9V6e1eaa"
   },
   "outputs": [],
   "source": [
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ebiu9ENaBXgs"
   },
   "source": [
    "# Abstract Summarizer\n",
    "The single sentence classifier defines the configuration of the model. Here, can be defined how the sentence representations are formed (cls token or pooling) and from which layer or layer combinations the representations are used (default is last hidden layer). Here, the classifier is the only decoder option.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6iRhoHQssEs"
   },
   "outputs": [],
   "source": [
    "class SingleSentClassifier(nn.Module):\n",
    "  def __init__(self):\n",
    "      super(SingleSentClassifier, self).__init__()\n",
    "      self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "      self.linear1 = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "      self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "      outputs = self.bert(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "      )\n",
    "      #top_vec = (1/3) * (outputs.hidden_states[1] + outputs.hidden_states[11] + outputs.hidden_states[12]) # combination of different layers\n",
    "      #top_vec = outputs.hidden_states[1] # single layer\n",
    "      top_vec = outputs.last_hidden_state # last hidden layer\n",
    "      sents_vec = pool_sents(top_vec, attention_mask)  #pooling\n",
    "      #sents_vec = top_vec[:, 0] #cls token\n",
    "      out = self.linear1(sents_vec).squeeze(-1)\n",
    "      out = self.sigmoid(out)\n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeO6WsX1HKVZ"
   },
   "outputs": [],
   "source": [
    "def pool_sents(top_vec, attention_mask):\n",
    "  sents_vec = torch.zeros([top_vec.size(0), top_vec.size(2)], dtype=torch.float32).to(device)\n",
    "  for s in range(top_vec.size(0)):\n",
    "    sent_len = torch.sum(attention_mask[s]).item() - 2\n",
    "    sents_vec[s] = torch.mean(top_vec[s, :sent_len], 0).to(device)\n",
    "  return sents_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7b9KEEdNqpId"
   },
   "outputs": [],
   "source": [
    "model = SingleSentClassifier()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_jcNWu7CT2H"
   },
   "outputs": [],
   "source": [
    "def length_loss(preds, labels):\n",
    "  \n",
    "  loss = (preds - labels)**2\n",
    "  if preds < labels:\n",
    "    loss = loss * 2\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmjdtBssCBLT"
   },
   "source": [
    "# Sentence Selection\n",
    "\n",
    "Here, the sentences for the summary of each abstract are selected based on the selection strategy:\n",
    "\n",
    "\n",
    "*   Best-3: select 3 highest scoring sentences\n",
    "*   Dynamic: select sentence based on threshold, then add/remove highest/lowest scoring sentences if the prediction was outside of the allowed sentence range (1-4)\n",
    "\n",
    "To change the strategy, uncomment the indicated lines and comment the lines for the other strategy (a bit messy, sorry). Because the abstracts are split into sentences, we need to put the sentences back together to form abstracts again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zYTV5OZSlunD"
   },
   "outputs": [],
   "source": [
    "def select_sents(sent_scores, labels, abstract_ids):\n",
    "\n",
    "  abs_accs = []\n",
    "  abs_prec = []\n",
    "  abs_rec = []\n",
    "  len_losses = []\n",
    "  preds_len = np.array([0,0,0,0])\n",
    "  gold_len = np.array([0,0,0,0])\n",
    "\n",
    "  for abs_id in torch.unique(abstract_ids):\n",
    "    abs_sents = (abstract_ids == abs_id).nonzero(as_tuple=True)[0].to(device)\n",
    "    abs_scores = sent_scores[abs_sents].to(device)\n",
    "    abs_labels = labels[abs_sents].to(device)\n",
    "\n",
    "    abs_scores = abs_scores.cpu().data.numpy()\n",
    "\n",
    "    # select best 3\n",
    "    # selected_sents = np.argsort(-abs_scores) \n",
    "    # selected_sents = selected_sents[:3]\n",
    "    # selected_sents = torch.tensor(selected_sents)\n",
    "    # ones = torch.ones(abs_scores.shape, dtype=torch.int64)\n",
    "    # selected_sents = torch.zeros(abs_scores.shape, dtype=torch.int64).scatter(0, selected_sents, ones).to(device)\n",
    "    # until here\n",
    "\n",
    "    # dynamic selection\n",
    "    selected_sents = (abs_scores > 0.5).astype(int)\n",
    "\n",
    "    if selected_sents.sum() == 0:\n",
    "     sorted_sents = np.argsort(-abs_scores)\n",
    "     selected_sents[sorted_sents[0]] = 1\n",
    "\n",
    "    elif selected_sents.sum() > 4:\n",
    "      sorted_sents = np.argsort(-abs_scores)\n",
    "      selected_sents = np.zeros(selected_sents.size, dtype=int)\n",
    "      for p in range(4):\n",
    "        selected_sents[sorted_sents[p]] = 1\n",
    "    \n",
    "    selected_sents = torch.tensor(selected_sents).to(device)\n",
    "    # until here\n",
    "\n",
    "    correct = 0\n",
    "    correct_pos = 0\n",
    "\n",
    "    for i in range(selected_sents.size(0)):\n",
    "      if selected_sents[i] == abs_labels[i]:\n",
    "        correct += 1\n",
    "        if abs_labels[i] == 1:\n",
    "          correct_pos += 1\n",
    "\n",
    "    acc = correct / selected_sents.size(0)\n",
    "    prec = correct_pos / torch.sum(selected_sents).item()\n",
    "    rec = correct_pos / torch.sum(abs_labels).item()\n",
    "    len_loss = length_loss(torch.sum(selected_sents, dtype=float).item(), torch.sum(abs_labels, dtype=float).item())\n",
    "    len_losses.append(len_loss)\n",
    "    preds_len[torch.sum(selected_sents, dtype=int).item()-1] += 1\n",
    "    gold_len[torch.sum(abs_labels, dtype=int).item()-1] += 1\n",
    "\n",
    "    for k in range(selected_sents.size(0)):\n",
    "      abs_accs.append(acc)\n",
    "      abs_prec.append(prec)\n",
    "      abs_rec.append(rec)\n",
    "\n",
    "  return np.mean(abs_accs), np.mean(abs_prec), np.mean(abs_rec), np.mean(len_losses), preds_len, gold_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmG5sGAdDkkL"
   },
   "source": [
    "# Training and validation\n",
    "\n",
    "Here the actual fine-tuning happens. First, the hyperparameters and some metrics are defined. Then the training and evaluation functions are defined and called. During the training process, multiple stats are returned at the end of every epoch including: loss, accuraccy, precision, eecall, f1 score, length Loss and the length distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDUdWuMImu8p"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 7\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.BCELoss(reduction=\"none\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RK39-JHLwNHA"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler):\n",
    "\n",
    "  model = model.train()\n",
    "\n",
    "  losses = []\n",
    "\n",
    "  for batch in train_data_loader:\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(device)\n",
    "    labels = batch[\"labels\"].to(device)\n",
    "    abstract_ids = batch[\"abstract_ids\"].to(device)\n",
    "\n",
    "    sent_scores = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "    loss = loss_fn(sent_scores, labels.float()).sum()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fx_KTd5UGclV"
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device):\n",
    "\n",
    "  model = model.eval()\n",
    "\n",
    "  all_abstract_ids = torch.empty([0]).to(device)\n",
    "  all_labels = torch.empty([0]).to(device)\n",
    "  all_sent_scores = torch.empty([0]).to(device)\n",
    "  losses = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "      input_ids = batch[\"input_ids\"].to(device)\n",
    "      attention_mask = batch[\"attention_mask\"].to(device)\n",
    "      labels = batch[\"labels\"].to(device)\n",
    "      abstract_ids = batch[\"abstract_ids\"].to(device)\n",
    "\n",
    "      sent_scores = model(\n",
    "          input_ids=input_ids,\n",
    "          attention_mask=attention_mask\n",
    "      )\n",
    "\n",
    "      loss = loss_fn(sent_scores, labels.float()).sum()\n",
    "      losses.append(loss.item())\n",
    "\n",
    "      all_abstract_ids = torch.cat((all_abstract_ids, abstract_ids))\n",
    "      all_labels = torch.cat((all_labels, labels))\n",
    "      all_sent_scores = torch.cat((all_sent_scores, sent_scores))\n",
    "\n",
    "    acc, prec, rec, len_losses, preds_len, gold_len = select_sents(all_sent_scores, all_labels, all_abstract_ids)\n",
    "\n",
    "  return np.mean(losses), acc, prec, rec, len_losses, preds_len, gold_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gDFLxk44ukLI"
   },
   "outputs": [],
   "source": [
    "def f1_score(prec, rec):\n",
    "  f1 = 2*((prec*rec)/(prec+rec))\n",
    "  # \"balanced f1:\"  2*((prec*rec)/(prec+rec)) - difference(prec, rec)*0.2\n",
    "  return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1a25btKd0Cd"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "best_f1 = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "  print('-' * 10)\n",
    "\n",
    "  train_loss = train_epoch(\n",
    "      model,\n",
    "      train_data_loader,\n",
    "      loss_fn,\n",
    "      optimizer,\n",
    "      device,\n",
    "      scheduler,\n",
    "  )\n",
    "\n",
    "  print(f'Train loss {round(train_loss, 4)}')\n",
    "\n",
    "\n",
    "  val_loss, val_acc, val_prec, val_rec, val_len_loss, preds_len, gold_len = eval_model(\n",
    "    model,\n",
    "    test_data_loader,\n",
    "    loss_fn,\n",
    "    device\n",
    "  )\n",
    "\n",
    "  f1 = f1_score(val_prec, val_rec)\n",
    "\n",
    "  print(f'Val loss {round(val_loss, 4)} accuracy {round(val_acc, 4)}')\n",
    "  print(f\"Val len loss {round(val_len_loss, 4)}\")\n",
    "  print(f\"Precision: {round(val_prec, 4)} Recall: {round(val_rec, 4)}\")\n",
    "  print(f\"F1-score: {round(f1, 4)}\")\n",
    "  print(f\"Predictions distribution: {preds_len}\")\n",
    "  print(f\"Labels distribution: {gold_len}\")\n",
    "  print()\n",
    "\n",
    "  if f1 > best_f1:\n",
    "    #torch.save(model.state_dict(), 'best_base_summarizer.bin')\n",
    "    best_f1 = f1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNkkSDSAM7mrG18v/r++cv0",
   "collapsed_sections": [],
   "mount_file_id": "1reK6Int_l6SVS7oC4qHLSDTa9mOWaCqb",
   "name": "Fine-Tuning Single Sent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
